{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges for week 6\n",
    "\n",
    "Now that we've seen how to run statistical testing and create supervised machine learning models in Python, it's time for you to apply this knowledge. This week has three challenges. Make sure to give it a try and complete all of them. \n",
    "\n",
    "**Some important notes for the challenges:**\n",
    "1. These challenges are a warming up, and help you get ready for class. Make sure to give them a try. If you get an error message, try to troubleshoot it (using Google often helps). If all else fails, go to the next challenge (but make sure to hand it in).\n",
    "2. While we of course like when you get all the answers right, the important thing is to exercise and apply the knowledge. So we will still accept challenges that may not be complete, as long as we see enough effort *for each challenge*. This means that if one of the challenges is not delivered (not started and no attempt shown), we unfortunately will not be able to provide a full grade for that week.\n",
    "3. Delivering the challenge to the right place is a critical part of the challenge. This means we will only be able to grade and accept challenges that are live on your own private GitHub repository (so with a link starting with https://github.com/uva-cw-digitalanalytics/2021s1-) **and** delivered on time as a Canvas assignment. Watch the videos on Canvas on how to hand in your challenges.\n",
    "\n",
    "### Facing issues? \n",
    "\n",
    "We are constantly monitoring the issues on the GitHub general repository (https://github.com/uva-cw-digitalanalytics/2021s1/issues) to help you out. Don't hesitate to log an issue there, explaining well what the problem is, showing the code you are using, and the error message you may be receiving. \n",
    "\n",
    "**Important:** We are only monitoring the repository in weekdays, and until 17.00. Issues logged after this time will most likely be answered the next day. This means you should now wait for our response before submitting a challenge :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting setup for the challenges\n",
    "\n",
    "We will use the Google Store data that we also saw in the video tutorials. Make sure to either have it by cloning the general repository, or downloading it from surfdrive (see link in the General Repository homepage) and placing it in the same folder as you are running this weekly challenge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The case\n",
    "Our website has launched new campaigns to increase in sales (as binary, converted from **order_euros**) and revenue (**order_euros**). We are now only interested in sales (i.e., binary DV).\n",
    "\n",
    "We are interested in two campaigns:\n",
    "* The **referral** campaign\n",
    "* The **CPC** campaign\n",
    "\n",
    "We want to know if (each campaign led to an increase in sales **compared** to the other campaigns (i.e., any traffic source that is not set as CPC or referral).\n",
    "\n",
    "### Important note:\n",
    "Because the dataset is very large and it may take some time to run the code, we will select a random sample of 10% of the visits that are in the dataset. Please run the code below (exactly as it is):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "visits = pd.read_pickle('googlestore_DA5weeklychallenges.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52308"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a sample of 10% visits. The \"random_state\" option ensures that everyone has the same data\n",
    "visits = visits.sample(frac=0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5231"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "Create two models for statistical testing (with statsmodels). One only with the campaign information (cpc and referral), and a second model with at least one additional independent or control variable. It can be one of the variables we showed in the DA6 tutorial, or another. Sales, as a binary variable (converted from order_euros) should be your DV.\n",
    "\n",
    "You need to:\n",
    "1. Justify your choice (in MarkDown) for the additional variable for the second model\n",
    "2. Create both models using statsmodels\n",
    "3. Interpret the models (i.e., what do they tell about the influence of the IV's in the DV)\n",
    "4. Indicate which model you'd choose, and why\n",
    "\n",
    "**IMPORTANT:** Don't forget to split your data in train / test datasets, and use only the train dataset here, as done also in the DA6 tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Using the LogisticRegression classifier from **sklearn**, recreate the same models that you created in statsmodels for challenge 1 using your training dataset. Then compare both models in terms of:\n",
    "1. How their confusion matrix looks like for the test dataset\n",
    "2. Their precision, recall and F1-score for the test dataset\n",
    "3. Indicate which model you'd choose, and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Using the DecisionTree classifier from **sklearn**, recreate the best performing model from challenge 2 (the one you indicated you'd choose). After the DecisionTree model is created, please:\n",
    "1. Create a chart for the decision tree\n",
    "2. Interpret the precision, recall and F1-score of this model for the test dataset (compared to the best model from challenge 2)\n",
    "3. Indicate which model you'd choose, and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
